{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "sample_SMILES = \"<s>C\"\n",
    "\n",
    "t = tokenizer(sample_SMILES, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "class BERT_GCxGC(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim, output_dim):\n",
    "        super(BERT_GCxGC, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        # Predicts Molecular Weight or M/Z\n",
    "        self.m_z = nn.Sequential(\n",
    "            nn.Linear(base_model.config.vocab_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(device)\n",
    "        # Predicting retention time 1\n",
    "        self.rt1 = nn.Sequential(\n",
    "            nn.Linear(base_model.config.vocab_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(device)\n",
    "        # Predicting retention time 2\n",
    "        self.rt2 = nn.Sequential(\n",
    "            nn.Linear(base_model.config.vocab_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs.logits[:, 0, :]  # Get the CLS token(<s> token)\n",
    "        m_z = self.m_z(cls_token)\n",
    "        rt1 = self.rt1(cls_token)\n",
    "        rt2 = self.rt2(cls_token)\n",
    "        return m_z, rt1, rt2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\").to(device)\n",
    "\n",
    "# Create the model with MLPs\n",
    "hidden_dim = 64  # Hidden dimension size for each MLP\n",
    "output_dim = 1  # Output dimension for each MLP\n",
    "custom_model = BERT_GCxGC(model, hidden_dim, output_dim).to(device)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# sample_SMILES = \"<s>CC1=CC(=CC=C1)S(=O)(=O)NC2=CC=C(C=C2)S(=O)(=O)NC(C)C\"\n",
    "# inputs = tokenizer(sample_SMILES, return_tensors=\"pt\")\n",
    "# outputs = custom_model(**inputs)\n",
    "# print(outputs)  # Outputs from the three MLPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = pd.read_csv('results/training_set_march20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 5384391.0\n",
      "Epoch 1 loss: 7054440.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Assuming you have a dataset in the form of a list of tuples [(smiles, m_z, rt1, rt2), ...]\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx]['Canonical_SMILES']\n",
    "        m_z = self.data.iloc[idx]['m_z']\n",
    "        rt1 = self.data.iloc[idx]['1st Dimension Time (s)']\n",
    "        rt2 = self.data.iloc[idx]['2nd Dimension Time (s)']\n",
    "\n",
    "        # Tokenize the SMILES and pad to the max length\n",
    "        inputs = self.tokenizer(smiles, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Remove the batch dimension that the tokenizer adds by default\n",
    "        input_ids = inputs.input_ids.squeeze(0)\n",
    "\n",
    "        # Your targets as a tensor\n",
    "        targets = torch.tensor([m_z, rt1, rt2], dtype=torch.float32, device=device)\n",
    "        \n",
    "        return input_ids.to(device), targets.to(device)\n",
    "\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "dataset = SMILESDataset(all_dataset, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(custom_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(2):\n",
    "    for input_ids, targets in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        m_z_pred, rt1_pred, rt2_pred = custom_model(input_ids)\n",
    "        loss = criterion(m_z_pred, targets[:, 0]) + criterion(rt1_pred, targets[:, 1]) + criterion(rt2_pred, targets[:, 2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "# torch.save(custom_model.state_dict(), \"chemberta_with_mlps_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
